{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Code\\\\NLP_Text_Summarize_Project\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Code\\\\NLP_Text_Summarize_Project'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "    root_dir: Path\n",
    "    STATUS_FILE: str\n",
    "    ALL_REQUIRED_FILES: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Text_Summarize.constants import *\n",
    "from src.Text_Summarize.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "\n",
    "    def get_validation_config(self) -> DataValidationConfig:\n",
    "        config = self.config.data_validation\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_validation_config = DataValidationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            STATUS_FILE=config.STATUS_FILE,\n",
    "            ALL_REQUIRED_FILES=config.ALL_REQUIRED_FILES\n",
    "        )\n",
    "\n",
    "        return data_validation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Text_Summarize import logger\n",
    "import re\n",
    "import langdetect\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class DataValidation:\n",
    "    def __init__(self, config: DataValidationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def validate_all_files_exist(self) -> bool:\n",
    "        #... (your existing code)\n",
    "\n",
    "    def validate_text_normalization(self) -> bool:\n",
    "        for file in self.config.ALL_REQUIRED_FILES:\n",
    "            with open(os.path.join(\"artifacts\", \"data_ingestion\", \"corpus\", file), 'r') as f:\n",
    "                text = f.read()\n",
    "                if not self.is_normalized(text):\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def is_normalized(self, text: str) -> bool:\n",
    "        # Implement text normalization checks (e.g., lowercase, no special characters)\n",
    "        pass\n",
    "\n",
    "    def validate_tokenization(self) -> bool:\n",
    "        for file in self.config.ALL_REQUIRED_FILES:\n",
    "            with open(os.path.join(\"artifacts\", \"data_ingestion\", \"corpus\", file), 'r') as f:\n",
    "                text = f.read()\n",
    "                if not self.is_tokenized(text):\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def is_tokenized(self, text: str) -> bool:\n",
    "        # Implement tokenization checks (e.g., words separated correctly)\n",
    "        pass\n",
    "\n",
    "    #... (add more validation methods for each step)\n",
    "\n",
    "    def validate_all(self) -> bool:\n",
    "        validation_status = True\n",
    "        validation_status &= self.validate_all_files_exist()\n",
    "        validation_status &= self.validate_text_normalization()\n",
    "        validation_status &= self.validate_tokenization()\n",
    "        #... (add more validation methods)\n",
    "        return validation_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class DataValidation:\n",
    "    def __init__(self, config: DataValidationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def validate_all_files_exist(self) -> bool:\n",
    "        try:\n",
    "            validation_status = None\n",
    "\n",
    "            all_files = os.listdir(os.path.join(\"artifacts\",\"data_ingestion\",\"corpus\"))\n",
    "\n",
    "            for file in all_files:\n",
    "                if file not in self.config.ALL_REQUIRED_FILES:\n",
    "                    validation_status = False\n",
    "                    with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                        f.write(f\"Validation status: {validation_status}\")\n",
    "            \n",
    "            return validation_status\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def validate_text_normalization(self, text):\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters and punctuation\n",
    "        text = re.sub(r\"\\s+\", \" \", text)  # Normalize whitespace characters\n",
    "        return text\n",
    "\n",
    "    def validate_tokenization(self, text):\n",
    "        tokens = word_tokenize(text)\n",
    "        if len(tokens) < 2:  # Check for minimum token length\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def validate_language_detection(self, text):\n",
    "        try:\n",
    "            lang = langdetect.detect(text)\n",
    "            return lang\n",
    "        except langdetect.lang_detect_exception.LangDetectException:\n",
    "            return \"unknown\"\n",
    "\n",
    "    def validate_text_length(self, text):\n",
    "        if len(text) < 10:  # Check for minimum text length\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def validate_stopwords(self, text):\n",
    "        tokens = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        if any(token in stop_words for token in tokens):  # Check for stopwords\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def validate_all(self) -> bool:\n",
    "        validation_status = True\n",
    "\n",
    "        for file in self.config.ALL_REQUIRED_FILES:\n",
    "            with open(os.path.join(\"artifacts\", \"data_ingestion\", \"corpus\", file), 'r') as f:\n",
    "                text = f.read()\n",
    "                text = self.validate_text_normalization(text)\n",
    "                validation_status &= self.validate_tokenization(text)\n",
    "                validation_status &= self.validate_language_detection(text)!= \"unknown\"\n",
    "                validation_status &= self.validate_text_length(text)\n",
    "                validation_status &= self.validate_stopwords(text)\n",
    "\n",
    "        return validation_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidation:\n",
    "    def __init__(self, config: DataValidationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def validate_all_files_exist(self) -> bool:\n",
    "        try:\n",
    "            all_files = os.listdir(os.path.join(\"artifacts\", \"data_ingestion\", \"corpus\"))\n",
    "            for file in all_files:\n",
    "                if file not in self.config.ALL_REQUIRED_FILES:\n",
    "                    with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                        f.write(\"Validation status: validate_all_files_exist - False\\n\")\n",
    "                    return False\n",
    "            with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                f.write(\"Validation status: validate_all_files_exist - True\\n\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def validate_text_normalization(self, text) -> str:\n",
    "        try:\n",
    "            text = text.lower()  # Convert to lowercase\n",
    "            text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters and punctuation\n",
    "            text = re.sub(r\"\\s+\", \" \", text)  # Normalize whitespace characters\n",
    "            with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                f.write(\"Validation status: validate_text_normalization - True\\n\")\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                f.write(f\"Validation status: validate_text_normalization - False ({str(e)})\\n\")\n",
    "            return \"\"\n",
    "\n",
    "    def validate_tokenization(self, text) -> bool:\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "            if len(tokens) < 2:  # Check for minimum token length\n",
    "                with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                    f.write(\"Validation status: validate_tokenization - False\\n\")\n",
    "                return False\n",
    "            with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                f.write(\"Validation status: validate_tokenization - True\\n\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                f.write(f\"Validation status: validate_tokenization - False ({str(e)})\\n\")\n",
    "            return False\n",
    "\n",
    "    def validate_language_detection(self, text) -> str:\n",
    "        try:\n",
    "            lang = langdetect.detect(text)\n",
    "            with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                f.write(f\"Validation status: validate_language_detection - {lang}\\n\")\n",
    "            return lang\n",
    "        except langdetect.lang_detect_exception.LangDetectException:\n",
    "            with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                f.write(\"Validation status: validate_language_detection - unknown\\n\")\n",
    "            return \"unknown\"\n",
    "\n",
    "    def validate_text_length(self, text) -> bool:\n",
    "        try:\n",
    "            if len(text) < 10:  # Check for minimum text length\n",
    "                with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                    f.write(\"Validation status: validate_text_length - False\\n\")\n",
    "                return False\n",
    "            with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                f.write(\"Validation status: validate_text_length - True\\n\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                f.write(f\"Validation status: validate_text_length - False ({str(e)})\\n\")\n",
    "            return False\n",
    "\n",
    "    def validate_stopwords(self, text) -> bool:\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "            if any(token in stop_words for token in tokens):  # Check for stopwords\n",
    "                with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                    f.write(\"Validation status: validate_stopwords - False\\n\")\n",
    "                return False\n",
    "            with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                f.write(\"Validation status: validate_stopwords - True\\n\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            with open(self.config.STATUS_FILE, 'a') as f:\n",
    "                f.write(f\"Validation status: validate_stopwords - False ({str(e)})\\n\")\n",
    "            return False\n",
    "\n",
    "    def validate_all(self) -> bool:\n",
    "        validation_status = True\n",
    "\n",
    "        for file in self.config.ALL_REQUIRED_FILES:\n",
    "            with open(os.path.join(\"artifacts\", \"data_ingestion\", \"corpus\", file), 'r') as f:\n",
    "                text = f.read()\n",
    "                text = self.validate_text_normalization(text)\n",
    "                if not self.validate_tokenization(text):\n",
    "                    validation_status = False\n",
    "                if self.validate_language_detection(text) == \"unknown\":\n",
    "                    validation_status = False\n",
    "                if not self.validate_text_length(text):\n",
    "                    validation_status = False\n",
    "                if not self.validate_stopwords(text):\n",
    "                    validation_status = False\n",
    "\n",
    "        return validation_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-18 14:12:10,642: INFO: common: yaml file: config\\config.yaml loaded succesfully]\n",
      "[2024-05-18 14:12:10,646: INFO: common: yaml file: params.yaml loaded succesfully]\n",
      "[2024-05-18 14:12:10,649: INFO: common: created directory at: artifacts]\n",
      "[2024-05-18 14:12:10,651: INFO: common: created directory at: artifacts/data_validation]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_validation_config = config.get_validation_config()\n",
    "    data_validation = DataValidation(config=data_validation_config)\n",
    "    data_validation.validate_all_files_exist()\n",
    "    data_validation.validate_all()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "potatoes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
